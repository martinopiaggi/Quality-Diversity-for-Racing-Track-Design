{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP-Elites track generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from ribs.archives import GridArchive\n",
    "from ribs.emitters import EmitterBase\n",
    "from ribs.schedulers import Scheduler\n",
    "from ribs.visualize import grid_archive_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://localhost:4242'\n",
    "POINTS_COUNT = 50\n",
    "MAX_SELECTED_CELLS = 10\n",
    "SOLUTION_DIM = POINTS_COUNT * 2 + MAX_SELECTED_CELLS * 2 + 1 \n",
    "TRACK_SIZE_RANGE = (2, 6)\n",
    "LENGTH_RANGE = (400, 2000)\n",
    "ITERATIONS = 500\n",
    "ARCHIVE_DIM = 3\n",
    "INIT_POPULATION = ARCHIVE_DIM * ARCHIVE_DIM \n",
    "\n",
    "DEBUG_CROSSOVER = True\n",
    "DEBUG_MUTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True, n_workers=5, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(iteration):\n",
    "    print(f\"Generating solution for iteration {iteration}\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{BASE_URL}/generate\",\n",
    "            json={\n",
    "                \"id\": iteration + random.random(),\n",
    "                \"mode\": \"voronoi\",\n",
    "                \"trackSize\": random.randint(TRACK_SIZE_RANGE[0], TRACK_SIZE_RANGE[1])\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error generating solution for iteration {iteration}: {e}\")\n",
    "        return None\n",
    "\n",
    "def solution_to_array(sol):\n",
    "    if sol is None:\n",
    "        return None\n",
    "    arr = np.zeros(SOLUTION_DIM)\n",
    "    for i, p in enumerate(sol.get(\"dataSet\", [])):\n",
    "        arr[i * 2] = p.get(\"x\", 0)\n",
    "        arr[i * 2 + 1] = p.get(\"y\", 0)\n",
    "    for i, c in enumerate(sol.get(\"selectedCells\", [])):\n",
    "        if i < MAX_SELECTED_CELLS:\n",
    "            idx = POINTS_COUNT * 2 + i * 2\n",
    "            arr[idx] = c.get(\"x\", 0)\n",
    "            arr[idx + 1] = c.get(\"y\", 0)\n",
    "    arr[-1] = sol.get(\"id\", 0)\n",
    "    return arr\n",
    "\n",
    "def array_to_solution(arr):\n",
    "    ds = []\n",
    "    for i in range(0, POINTS_COUNT * 2, 2):\n",
    "        ds.append({\"x\": float(arr[i]), \"y\": float(arr[i+1])})\n",
    "    sel = []\n",
    "    for i in range(POINTS_COUNT * 2, SOLUTION_DIM - 1, 2):\n",
    "        x_val = arr[i]\n",
    "        y_val = arr[i+1]\n",
    "        if x_val != 0 or y_val != 0:\n",
    "            sel.append({\"x\": float(x_val), \"y\": float(y_val)})\n",
    "    return {\n",
    "        \"id\": float(arr[-1]),\n",
    "        \"mode\": \"voronoi\",\n",
    "        \"dataSet\": ds,\n",
    "        \"selectedCells\": sel\n",
    "    }\n",
    "\n",
    "def get_fractional_part(x):\n",
    "    return x - int(x)\n",
    "\n",
    "def evaluate_solution(sol):\n",
    "    solution_id = sol.get(\"id\", 0)\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{BASE_URL}/evaluate\",\n",
    "            json=sol,\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        fit = data.get(\"fitness\", {})\n",
    "        s = fit.get(\"speed_entropy\", 0)\n",
    "        c = fit.get(\"curvature_entropy\", 0)\n",
    "        g = fit.get(\"gaps_mean\", 0)\n",
    "        if not all(isinstance(x, (int, float)) for x in [s, c, g]):\n",
    "            return solution_id, False, \"Invalid fitness values\", -9999, [0, 0]\n",
    "        score = s + c - (0.01 * g)\n",
    "        return solution_id, True, \"\", score, [s, g]\n",
    "    except (requests.RequestException, ValueError) as e:\n",
    "        return solution_id, False, str(e), -9999, [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmitter(EmitterBase):\n",
    "    def __init__(self, archive, solution_dim, batch_size=ARCHIVE_DIM, bounds=None):\n",
    "        super().__init__(archive, solution_dim=solution_dim, bounds=bounds)\n",
    "        self.batch_size = batch_size\n",
    "        self.iteration = 0\n",
    "\n",
    "    def ask(self):\n",
    "        self.iteration += 1\n",
    "        print(f\"Emitter.ask() called for iteration {self.iteration}\")\n",
    "        if self.iteration <= INIT_POPULATION:\n",
    "            out = []\n",
    "            for _ in range(self.batch_size):\n",
    "                sol = generate_solution(self.iteration - 1)\n",
    "                arr = solution_to_array(sol)\n",
    "                if arr is not None:\n",
    "                    out.append(arr)\n",
    "                else:\n",
    "                    out.append(np.full(SOLUTION_DIM, -9999))\n",
    "            return np.array(out)\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                return self.mutate_solutions()\n",
    "            else:\n",
    "                return self.crossover_solutions()\n",
    "\n",
    "    def mutate_solutions(self):\n",
    "        print(f\"Mutating solutions for iteration {self.iteration}\")\n",
    "        parents = self.archive.sample_elites(self.batch_size)\n",
    "        out = []\n",
    "        for i in range(self.batch_size):\n",
    "            arr = parents[\"solution\"][i]\n",
    "            sol = array_to_solution(arr)\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    f\"{BASE_URL}/mutate\",\n",
    "                    json={\n",
    "                        \"individual\": sol,\n",
    "                        \"intensityMutation\": 10\n",
    "                    },\n",
    "                    timeout=60\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                mutated = response.json().get(\"mutated\", {})\n",
    "                frac = get_fractional_part(sol[\"id\"])\n",
    "                mutated[\"id\"] = self.iteration - 1 + frac\n",
    "                mutated_arr = solution_to_array(mutated)\n",
    "                if mutated_arr is not None:\n",
    "                    out.append(mutated_arr)\n",
    "                    print(f\"Mutated ID={sol['id']} to ID={mutated['id']}\")\n",
    "                else:\n",
    "                    out.append(np.full(SOLUTION_DIM, -9999))\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error mutating solution ID={sol['id']}: {e}\")\n",
    "                out.append(np.full(SOLUTION_DIM, -9999))\n",
    "        return np.array(out)\n",
    "\n",
    "    def crossover_solutions(self):\n",
    "        print(f\"Crossover solutions for iteration {self.iteration}\")\n",
    "        out = []\n",
    "        for _ in range(self.batch_size // 2):\n",
    "            try:\n",
    "                while True:\n",
    "                    parents = self.archive.sample_elites(2)\n",
    "                    sol1 = array_to_solution(parents[\"solution\"][0])\n",
    "                    sol2 = array_to_solution(parents[\"solution\"][1])\n",
    "                    if sol1[\"id\"] != sol2[\"id\"]:\n",
    "                        break\n",
    "                response = requests.post(\n",
    "                    f\"{BASE_URL}/crossover\",\n",
    "                    json={\n",
    "                        \"mode\": \"voronoi\",\n",
    "                        \"parent1\": sol1,\n",
    "                        \"parent2\": sol2\n",
    "                    },\n",
    "                    timeout=60\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                offspring = response.json().get(\"offspring\", {})\n",
    "                f1 = get_fractional_part(sol1[\"id\"])\n",
    "                f2 = get_fractional_part(sol2[\"id\"])\n",
    "                frac = (f1 + f2) % 1\n",
    "                child_id = self.iteration - 1 + frac\n",
    "                child_sol = {\n",
    "                    \"id\": child_id,\n",
    "                    \"mode\": \"voronoi\",\n",
    "                    \"trackSize\": len(offspring.get(\"sel\", [])),\n",
    "                    \"dataSet\": offspring.get(\"ds\", []),\n",
    "                    \"selectedCells\": offspring.get(\"sel\", [])\n",
    "                }\n",
    "                child_arr = solution_to_array(child_sol)\n",
    "                if child_arr is not None:\n",
    "                    out.append(child_arr)\n",
    "                    print(f\"Crossover Parent1 ID={sol1['id']}, Parent2 ID={sol2['id']} => Child ID={child_id}\")\n",
    "                else:\n",
    "                    out.append(np.full(SOLUTION_DIM, -9999))\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error during crossover: {e}\")\n",
    "                out.append(np.full(SOLUTION_DIM, -9999))\n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illuminating search spaces by mapping elites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = GridArchive(\n",
    "    solution_dim=SOLUTION_DIM,\n",
    "    dims=[ARCHIVE_DIM, ARCHIVE_DIM],\n",
    "    ranges=[(1,4), (0, 30)]\n",
    ")\n",
    "emitter = CustomEmitter(\n",
    "    archive,\n",
    "    solution_dim=SOLUTION_DIM,\n",
    "    batch_size=INIT_POPULATION,\n",
    "    bounds=[(0, 600)] * (SOLUTION_DIM - 1) + [(0, float('inf'))]\n",
    ")\n",
    "scheduler = Scheduler(archive, [emitter])\n",
    "\n",
    "def run_map_elites(iters):\n",
    "    global_best_score = -9999\n",
    "    global_best_id = None\n",
    "    for i in range(iters):\n",
    "        print(f\"=== Starting iteration {i+1} ===\")\n",
    "        try:\n",
    "            sols = scheduler.ask()\n",
    "            sol_dicts = [array_to_solution(s) for s in sols]\n",
    "            results = client.map(evaluate_solution, sol_dicts)\n",
    "            gathered = client.gather(results)\n",
    "            \n",
    "            obj_list = []\n",
    "            meas_list = []\n",
    "            failed_ids = []\n",
    "            for res in gathered:\n",
    "                sol_id, success, msg, score, measures = res\n",
    "                if not success or not np.isfinite(score):\n",
    "                    print(f\"Warning: clamping {score} to -9999 for solution ID={sol_id}, reason: {msg}\")\n",
    "                    score = -9999\n",
    "                    failed_ids.append(sol_id)\n",
    "                else:\n",
    "                    print(f\"Solution ID={sol_id} evaluated with score={score:.2f}\")\n",
    "                    # Update global best if necessary\n",
    "                    if score > global_best_score:\n",
    "                        global_best_score = score\n",
    "                        global_best_id = sol_id\n",
    "                obj_list.append(score)\n",
    "                meas_list.append(measures)\n",
    "            \n",
    "            scheduler.tell(obj_list, meas_list)\n",
    "            \n",
    "            batch_best = max(obj_list) if obj_list else -9999\n",
    "            print(f\"Iteration {i+1} ended. Best in batch = {batch_best:.2f}\")\n",
    "            if global_best_id is not None:\n",
    "                print(f\"Global Best Score so far: {global_best_score:.2f} (ID={global_best_id})\")\n",
    "            \n",
    "            data = archive.data()\n",
    "            if len(data) > 0:\n",
    "                arch_obj = data[\"objective\"]\n",
    "                mean_val = np.mean(arch_obj)\n",
    "                best_val = np.max(arch_obj)\n",
    "                cov = archive.stats.coverage\n",
    "                print(f\"Archive size={len(archive)}, Coverage={cov:.3f}, Mean={mean_val:.2f}, Best={best_val:.2f}\")\n",
    "            else:\n",
    "                print(\"Archive empty so far\")\n",
    "            \n",
    "            if failed_ids:\n",
    "                print(f\"Failed evaluations for solution IDs: {failed_ids}\")\n",
    "            \n",
    "            # Plot every 5 iterations\n",
    "            if (i + 1) % 5 == 0:\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                grid_archive_heatmap(archive)\n",
    "                plt.title(f\"Archive Heatmap - Iteration {i+1}\")\n",
    "                plt.xlabel(\"Speed Entropy\")\n",
    "                plt.ylabel(\"Mean Gaps\")\n",
    "                plt.savefig(f\"archive_heatmap_iter_{i+1}.png\")\n",
    "                plt.close()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {i+1}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_map_elites(ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All iterations complete.\")\n",
    "print(f\"Final archive size={len(archive)}, Coverage={archive.stats.coverage:.3f}\")\n",
    "plt.figure(figsize=(6,5))\n",
    "grid_archive_heatmap(archive)\n",
    "plt.title(\"Final Archive Heatmap\")\n",
    "plt.xlabel(\"Speed Entropy\")\n",
    "plt.ylabel(\"Mean Gaps\")\n",
    "plt.savefig(\"final_archive_heatmap.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
