{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP-Elites track generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib    \n",
    "# Just load a saved t-SNE or UMAP model     \n",
    "EMBEDDING_MODEL = joblib.load(\"embedding/umap_model.joblib\")   # 2-D model trained offline\n",
    "\n",
    "\n",
    "from ribs.archives import SlidingBoundariesArchive\n",
    "from ribs.emitters import EmitterBase\n",
    "from ribs.schedulers import Scheduler\n",
    "from dask.distributed import Client, LocalCluster, as_completed\n",
    "from ribs.visualize import sliding_boundaries_archive_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://localhost:4242'\n",
    "GENERATION_MODE = 'voronoi' #'convexHull' or 'voronoi'\n",
    "POINTS_COUNT = 100\n",
    "MAX_SELECTED_CELLS = 10 # relevant only for voronoi\n",
    "SOLUTION_DIM = POINTS_COUNT * 2 + MAX_SELECTED_CELLS * 2 + 1 \n",
    "TRACK_SIZE_RANGE = (4,10) #(4, 10) for voronoi otherwise (100,100)\n",
    "ITERATIONS = 1000\n",
    "ARCHIVE_DIM = 10\n",
    "INIT_POPULATION = ARCHIVE_DIM * ARCHIVE_DIM \n",
    "\n",
    "BATCH_SIZE=10\n",
    "\n",
    "INVALID_SCORE    = -1e9\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "DEBUG_CROSSOVER = True\n",
    "DEBUG_MUTATION = True\n",
    "\n",
    "\n",
    "ARCHIVE_BINS = 30                               # cells per axis\n",
    "REMAPPING_EVERY = 200                           # move boundaries every 200 insertions\n",
    "BUFFER_SIZE = 1000                              # keep last 1000 solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True, n_workers=BATCH_SIZE, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(iteration):\n",
    "    print(f\"Generating solution for iteration {iteration}\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{BASE_URL}/generate\",\n",
    "            json={\n",
    "                \"id\": iteration + random.random(),\n",
    "                \"mode\": GENERATION_MODE,\n",
    "                \"trackSize\": random.randint(TRACK_SIZE_RANGE[0], TRACK_SIZE_RANGE[1])\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error generating solution for iteration {iteration}: {e}\")\n",
    "        return None\n",
    "\n",
    "def solution_to_array(sol):\n",
    "    if sol is None:\n",
    "        return None\n",
    "    arr = np.zeros(SOLUTION_DIM)\n",
    "    for i, p in enumerate(sol.get(\"dataSet\", [])):\n",
    "        arr[i * 2] = p.get(\"x\", 0)\n",
    "        arr[i * 2 + 1] = p.get(\"y\", 0)\n",
    "    for i, c in enumerate(sol.get(\"selectedCells\", [])):\n",
    "        if i < MAX_SELECTED_CELLS:\n",
    "            idx = POINTS_COUNT * 2 + i * 2\n",
    "            arr[idx] = c.get(\"x\", 0)\n",
    "            arr[idx + 1] = c.get(\"y\", 0)\n",
    "    arr[-1] = sol.get(\"id\", 0)\n",
    "    return arr\n",
    "\n",
    "def array_to_solution(arr):\n",
    "    ds = []\n",
    "    for i in range(0, POINTS_COUNT * 2, 2):\n",
    "        ds.append({\"x\": float(arr[i]), \"y\": float(arr[i+1])})\n",
    "    sel = []\n",
    "    for i in range(POINTS_COUNT * 2, SOLUTION_DIM - 1, 2):\n",
    "        x_val = arr[i]\n",
    "        y_val = arr[i+1]\n",
    "        if x_val != 0 or y_val != 0:\n",
    "            sel.append({\"x\": float(x_val), \"y\": float(y_val)})\n",
    "    return {\n",
    "        \"id\": float(arr[-1]),\n",
    "        \"mode\": GENERATION_MODE,\n",
    "        \"dataSet\": ds,\n",
    "        \"selectedCells\": sel\n",
    "    }\n",
    "\n",
    "def get_fractional_part(x):\n",
    "    return x - int(x)\n",
    "\n",
    "def pca_align(points):\n",
    "    pts = points - points.mean(0)\n",
    "    u, _, _ = np.linalg.svd(pts, full_matrices=False)\n",
    "    angle = np.arctan2(u[1, 0], u[0, 0])\n",
    "    rot = np.array([[np.cos(-angle), -np.sin(-angle)],\n",
    "                    [np.sin(-angle),  np.cos(-angle)]])\n",
    "    aligned = pts @ rot.T\n",
    "    if aligned[0, 0] < 0:\n",
    "        aligned[:, 0] *= -1\n",
    "    return aligned\n",
    "\n",
    "def descriptor_from_track(sol):\n",
    "    pts = np.array([[p[\"x\"], p[\"y\"]] for p in sol.get(\"splineVector\", [])], dtype=float)\n",
    "    aligned = pca_align(pts)\n",
    "    flat = aligned.ravel()\n",
    "    return EMBEDDING_MODEL.transform(flat[None, :])[0]\n",
    "\n",
    "_STATS = {\n",
    "    'len_min':  np.inf, 'len_max':  -np.inf,\n",
    "    'ov_min':   np.inf, 'ov_max':   -np.inf,\n",
    "    'dx_min':   np.inf, 'dx_max':   -np.inf,\n",
    "    'bend_min': np.inf, 'bend_max': -np.inf,\n",
    "}\n",
    "\n",
    "def _upd(k, v):\n",
    "    lo, hi = f'{k}_min', f'{k}_max'\n",
    "    if v < _STATS[lo]: _STATS[lo] = v\n",
    "    if v > _STATS[hi]: _STATS[hi] = v\n",
    "\n",
    "def _norm(k, v, eps=1e-6):\n",
    "    lo, hi = _STATS[f'{k}_min'], _STATS[f'{k}_max']\n",
    "    return (v - lo) / (hi - lo + eps)\n",
    "\n",
    "def fitness_formula(fit):\n",
    "    length     = max(fit.get('length', 0.0), 1e-3)\n",
    "    bend_len   = fit.get('right_len', 0.0) + fit.get('left_len', 0.0)\n",
    "    overtakes  = fit.get('total_overtakes', 0.0)\n",
    "    dx         = abs(fit.get('deltaX', 0.0)) or 1e-3\n",
    "\n",
    "    bend_ratio = bend_len / length\n",
    "\n",
    "    for k, v in (('len', length), ('ov', overtakes), ('dx', dx),\n",
    "                 ('bend', bend_ratio)):\n",
    "        _upd(k, v)\n",
    "\n",
    "    score = (\n",
    "        0.25 * _norm('len',  length)         +  # encourage longer tracks\n",
    "        0.60 * _norm('bend', bend_ratio)     +  # maximise curves per metre\n",
    "        0.15 * (_norm('ov',  overtakes) /\n",
    "                (_norm('dx', dx) + 1e-3))       # overtakes, damped by Δx\n",
    "    )\n",
    "    return float(score)\n",
    "\n",
    "\n",
    "def evaluate_solution(sol):\n",
    "    sol_id = sol.get(\"id\", 0)\n",
    "    ok = True\n",
    "    msg = \"\"\n",
    "    try:\n",
    "        r = requests.post(f\"{BASE_URL}/evaluate\", json=sol, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        r_json = r.json()\n",
    "        fit = r_json.get(\"fitness\", {})\n",
    "        desc = descriptor_from_track(r_json)  \n",
    "    except Exception as e:\n",
    "        ok = False \n",
    "        msg = str(e)\n",
    "        desc = np.zeros((2,))  # just in case of error\n",
    "        fit = INVALID_SCORE\n",
    "\n",
    "    return sol_id, ok, msg, fit, desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmitter(EmitterBase):\n",
    "    def __init__(self, archive, solution_dim, batch_size=BATCH_SIZE, bounds=None):\n",
    "        super().__init__(archive, solution_dim=solution_dim, bounds=bounds)\n",
    "        self.batch_size = batch_size\n",
    "        self.iteration = 0\n",
    "\n",
    "    def ask(self):\n",
    "        self.iteration += 1\n",
    "        print(f\"Emitter.ask() called for iteration {self.iteration}\")\n",
    "        if self.iteration <= INIT_POPULATION:\n",
    "            out = []\n",
    "            for _ in range(self.batch_size):\n",
    "                sol = generate_solution(self.iteration - 1)\n",
    "                arr = solution_to_array(sol)\n",
    "                if arr is not None:\n",
    "                    out.append(arr)\n",
    "                else:\n",
    "                    out.append(np.full(SOLUTION_DIM, INVALID_SCORE))\n",
    "            return np.array(out)\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                return self.mutate_solutions()\n",
    "            else:\n",
    "                return self.crossover_solutions()\n",
    "\n",
    "    def mutate_solutions(self):\n",
    "        print(f\"Mutating solutions for iteration {self.iteration}\")\n",
    "        parents = self.archive.sample_elites(self.batch_size)\n",
    "        out = []\n",
    "        for i in range(self.batch_size):\n",
    "            arr = parents[\"solution\"][i]\n",
    "            sol = array_to_solution(arr)\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    f\"{BASE_URL}/mutate\",\n",
    "                    json={\n",
    "                        \"individual\": sol,\n",
    "                        \"intensityMutation\": 20\n",
    "                    },\n",
    "                    timeout=60\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                mutated = response.json().get(\"mutated\", {})\n",
    "                frac = get_fractional_part(sol[\"id\"])\n",
    "                mutated[\"id\"] = self.iteration - 1 + frac\n",
    "                mutated_arr = solution_to_array(mutated)\n",
    "                if mutated_arr is not None:\n",
    "                    out.append(mutated_arr)\n",
    "                    print(f\"Mutated ID={sol['id']} to ID={mutated['id']}\")\n",
    "                else:\n",
    "                    out.append(np.full(SOLUTION_DIM, INVALID_SCORE))\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error mutating solution ID={sol['id']}: {e}\")\n",
    "                out.append(np.full(SOLUTION_DIM, INVALID_SCORE))\n",
    "        return np.array(out)\n",
    "\n",
    "    def crossover_solutions(self):\n",
    "        print(f\"Crossover solutions for iteration {self.iteration}\")\n",
    "        out = []\n",
    "        for _ in range(self.batch_size // 2):\n",
    "            try:\n",
    "                while True:\n",
    "                    parents = self.archive.sample_elites(2)\n",
    "                    sol1 = array_to_solution(parents[\"solution\"][0])\n",
    "                    sol2 = array_to_solution(parents[\"solution\"][1])\n",
    "                    if sol1[\"id\"] != sol2[\"id\"]:\n",
    "                        break\n",
    "                response = requests.post(\n",
    "                    f\"{BASE_URL}/crossover\",\n",
    "                    json={\n",
    "                        \"mode\": GENERATION_MODE,\n",
    "                        \"parent1\": sol1,\n",
    "                        \"parent2\": sol2\n",
    "                    },\n",
    "                    timeout=60\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                offspring = response.json().get(\"offspring\", {})\n",
    "                f1 = get_fractional_part(sol1[\"id\"])\n",
    "                f2 = get_fractional_part(sol2[\"id\"])\n",
    "                frac = (f1 + f2) % 1\n",
    "                child_id = self.iteration - 1 + frac\n",
    "                child_sol = {\n",
    "                    \"id\": child_id,\n",
    "                    \"mode\": GENERATION_MODE,\n",
    "                    \"trackSize\": len(offspring.get(\"sel\", [])),\n",
    "                    \"dataSet\": offspring.get(\"ds\", []),\n",
    "                    \"selectedCells\": offspring.get(\"sel\", [])\n",
    "                }\n",
    "                child_arr = solution_to_array(child_sol)\n",
    "                if child_arr is not None:\n",
    "                    out.append(child_arr)\n",
    "                    print(f\"Crossover Parent1 ID={sol1['id']}, Parent2 ID={sol2['id']} => Child ID={child_id}\")\n",
    "                else:\n",
    "                    out.append(np.full(SOLUTION_DIM, INVALID_SCORE))\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error during crossover: {e}\")\n",
    "                out.append(np.full(SOLUTION_DIM, INVALID_SCORE))\n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illuminating search spaces by mapping elites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "archive = SlidingBoundariesArchive(\n",
    "    solution_dim=SOLUTION_DIM,\n",
    "    dims=[ARCHIVE_BINS, ARCHIVE_BINS],\n",
    "    ranges=[(-1, 1), (-1, 1)],\n",
    "    remap_frequency=REMAPPING_EVERY,\n",
    "    buffer_capacity=BUFFER_SIZE\n",
    ")\n",
    "\n",
    "emitter = CustomEmitter(\n",
    "    archive,\n",
    "    solution_dim=SOLUTION_DIM,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bounds=[(0, 600)] * (SOLUTION_DIM - 1) + [(0, float(\"inf\"))]\n",
    ")\n",
    "\n",
    "scheduler = Scheduler(archive, [emitter])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Resume from latest pickle checkpoint if one exists\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "checkpoints = sorted(glob.glob(\"checkpoint_*.pkl\"))\n",
    "start_iter = 0\n",
    "global_best_score = INVALID_SCORE\n",
    "global_best_id = None\n",
    "\n",
    "if checkpoints:\n",
    "    latest_ckpt = checkpoints[-1]\n",
    "    with open(latest_ckpt, \"rb\") as f:\n",
    "        state = pickle.load(f)\n",
    "    scheduler           = state[\"scheduler\"]\n",
    "    archive  = scheduler.archive\n",
    "    start_iter          = state[\"iteration\"]\n",
    "    global_best_score   = state[\"global_best_score\"]\n",
    "    global_best_id      = state[\"global_best_id\"]\n",
    "    print(f\"[Resume] Loaded {latest_ckpt}, resuming from iteration {start_iter+1}\")\n",
    "else:\n",
    "    print(\"[Resume] No checkpoint found — starting fresh.\")\n",
    "\n",
    "\n",
    "def run_map_elites(total_iters, start_iter=0):\n",
    "    # declare that we mean to update these module‐level variables\n",
    "    global global_best_score, global_best_id\n",
    "\n",
    "    for i in range(start_iter, total_iters):\n",
    "        print(f\"=== Starting iteration {i+1} ===\")\n",
    "        sols      = scheduler.ask()\n",
    "        sol_dicts = [array_to_solution(s) for s in sols]\n",
    "        futs      = [client.submit(evaluate_solution, sol) for sol in sol_dicts]\n",
    "        gathered  = [f.result() for f in as_completed(futs)]\n",
    "\n",
    "        obj_list, clean = [], []\n",
    "        for sol_id, ok, msg, fit, desc in gathered:\n",
    "            score = INVALID_SCORE\n",
    "            if not ok:\n",
    "                print(f\"Warning: clamping bad score for ID={sol_id} ({msg})\")\n",
    "            else:\n",
    "                score = fitness_formula(fit)\n",
    "                print(f\"Solution ID={sol_id} evaluated with score={score:.2f}\")\n",
    "                if score > global_best_score:\n",
    "                    global_best_score = score\n",
    "                    global_best_id    = sol_id\n",
    "            clean.append((score, desc))\n",
    "            obj_list.append(score)\n",
    "\n",
    "        obj_batch, meas_batch = zip(*clean)\n",
    "        scheduler.tell(list(obj_batch), list(meas_batch))\n",
    "\n",
    "        batch_best = max(obj_list) if obj_list else INVALID_SCORE\n",
    "        print(f\"Iteration {i+1} ended. Best in batch = {batch_best:.2f}\")\n",
    "        print(f\"Global best so far: {global_best_score:.2f} (ID={global_best_id})\")\n",
    "\n",
    "        # ── Stats ──\n",
    "        data = archive.data()\n",
    "        if data:\n",
    "            arch_obj = data[\"objective\"]\n",
    "            valid    = arch_obj != INVALID_SCORE\n",
    "            mean_val = np.mean(arch_obj[valid]) if np.any(valid) else 0.0\n",
    "            best_val = np.max(arch_obj[valid]) if np.any(valid) else 0.0\n",
    "            cov      = archive.stats.coverage\n",
    "            print(f\"Archive size={len(archive)}, cov={cov:.3f}, mean={mean_val:.2f}, best={best_val:.2f}\")\n",
    "        else:\n",
    "            print(\"Archive still empty\")\n",
    "\n",
    "        # ── Checkpoint ──\n",
    "        if (i + 1) % 50 == 0:\n",
    "            ckpt_name = f\"checkpoint_{i+1:04d}.pkl\"\n",
    "            with open(ckpt_name, \"wb\") as f:\n",
    "                pickle.dump({\n",
    "                    \"scheduler\":         scheduler,\n",
    "                    \"iteration\":         i+1,\n",
    "                    \"global_best_score\": global_best_score,\n",
    "                    \"global_best_id\":    global_best_id\n",
    "                }, f)\n",
    "            print(f\"[Checkpoint] Saved {ckpt_name}\")\n",
    "\n",
    "         # ── Plot every 5 iterations ─────────────────────────────────\n",
    "        if (i + 1) % 10 == 0:\n",
    "            # Pull objective array from archive data\n",
    "            arch_obj = np.array(archive.data()[\"objective\"])\n",
    "            valid    = arch_obj != INVALID_SCORE\n",
    "\n",
    "            if np.any(valid):\n",
    "                # Compute percentiles to clip outliers\n",
    "                p_low, p_high = np.percentile(arch_obj[valid], [20, 80])\n",
    "            else:\n",
    "                p_low = p_high = 0.0\n",
    "\n",
    "            fig = plt.figure(figsize=(8, 6))\n",
    "            sliding_boundaries_archive_heatmap(\n",
    "                archive,\n",
    "                boundary_lw=0.5,\n",
    "                vmin=p_low,\n",
    "                vmax=p_high,\n",
    "            )\n",
    "            plt.title(f\"Archive Heat-map – Iteration {i+1}\")\n",
    "            plt.xlabel(\"UMAP-1\")\n",
    "            plt.ylabel(\"UMAP-2\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"archive_heatmap_iter_{i+1}.png\")\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_map_elites(ITERATIONS, start_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final archive size={len(archive)}, Coverage={archive.stats.coverage:.3f}\")\n",
    "size      = len(archive)\n",
    "coverage  = archive.stats.coverage\n",
    "objs      = archive.data()[\"objective\"]\n",
    "data      = archive.data()\n",
    "\n",
    "print(f\"Archive size     : {size}\")\n",
    "print(f\"Coverage         : {coverage:.3%}\")\n",
    "print(f\"Best objective   : {objs.max():.3f}\")\n",
    "print(f\"Mean objective   : {objs.mean():.3f}\\n\")\n",
    "\n",
    "k = 10                                        # how many to list\n",
    "for i in range(min(k, size)):\n",
    "    sol_vec = data[\"solution\"][i]\n",
    "    sol_id  = sol_vec[-1]                     # float ID\n",
    "    umap_xy = data[\"measures\"][i]\n",
    "    print(f\"{i+1:2d}. \"\n",
    "          f\"ID={sol_id:.17f}   \"\n",
    "          f\"UMAP=({umap_xy[0]:7.3f}, {umap_xy[1]:7.3f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_x = 10.0   # change this as needed\n",
    "k        = 4     # how many neighbours to display\n",
    "\n",
    "measures = data[\"measures\"]                    # (N, 2) array\n",
    "dx       = np.abs(measures[:, 0] - target_x)   # distance to target on x-axis\n",
    "\n",
    "nearest_idx = np.argsort(dx)[:k]               # indices of the k closest\n",
    "for rank, idx in enumerate(nearest_idx, 1):\n",
    "    sol_vec = data[\"solution\"][idx]\n",
    "    sol_id  = sol_vec[-1]\n",
    "    umap_xy = measures[idx]\n",
    "    score   = objs[idx]\n",
    "    print(f\"{rank:2d}. \"\n",
    "          f\"ID={sol_id:.17f}   \"\n",
    "          f\"dx={dx[idx]:7.3f}   \"\n",
    "          f\"score={score:8.3f}   \"\n",
    "          f\"UMAP=({umap_xy[0]:7.3f}, {umap_xy[1]:7.3f})\")\n",
    "    \n",
    "measures   = data[\"measures\"]            # (N, 2) array of UMAP coordinates\n",
    "all_x, all_y = measures[:, 0], measures[:, 1]\n",
    "\n",
    "# colour entire archive by objective score\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(all_x, all_y,\n",
    "                 c=objs,                # colour scale = performance\n",
    "                 s=12,                  # dot size\n",
    "                 alpha=0.8,\n",
    "                 cmap=\"viridis\")\n",
    "\n",
    "# highlight the nearest k in red with black edge\n",
    "highlight = measures[nearest_idx]\n",
    "plt.scatter(highlight[:, 0], highlight[:, 1],\n",
    "            facecolors='none',\n",
    "            edgecolors='red',\n",
    "            s=80,\n",
    "            linewidths=1.5,\n",
    "            label=f\"{len(nearest_idx)} closest to UMAP-x={target_x}\")\n",
    "\n",
    "plt.colorbar(sc, label=\"Objective value\")\n",
    "plt.title(\"Archive in UMAP space\\n(highlighted = closest to target x)\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 15                                   # how many random elites to inspect\n",
    "rng = np.random.default_rng(42)          # reproducible\n",
    "rand_idx = rng.choice(len(archive), size=m, replace=False)\n",
    "\n",
    "samples = []                             # keep the decoded solutions here\n",
    "for j, idx in enumerate(rand_idx, 1):\n",
    "    sol_vec = data[\"solution\"][idx]\n",
    "    sol_id  = sol_vec[-1]\n",
    "    umap_xy = data[\"measures\"][idx]\n",
    "    score   = objs[idx]\n",
    "    print(f\"{j:2d}. ID={sol_id:.17f}   \"\n",
    "          f\"score={score:8.3f}   \"\n",
    "          f\"UMAP=({umap_xy[0]:7.3f}, {umap_xy[1]:7.3f})\")\n",
    "    \n",
    "    # store full dict in `samples` for later, if desired\n",
    "    samples.append(array_to_solution(sol_vec))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "#   filter out objective outliers (beyond ±3 σ from the mean)\n",
    "# --------------------------------------------------------------\n",
    "obj_mean = objs.mean()\n",
    "obj_std  = objs.std()\n",
    "lo, hi   = obj_mean - 3*obj_std, obj_mean + 3*obj_std\n",
    "mask     = (objs >= lo) & (objs <= hi)\n",
    "\n",
    "x_f, y_f = all_x[mask], all_y[mask]\n",
    "w_f      = objs[mask]                       # colour = objective value\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "hb = plt.hist2d(x_f, y_f,\n",
    "                bins=50,\n",
    "                weights=w_f,\n",
    "                cmap=\"plasma\",\n",
    "                vmin=obj_mean - obj_std,    # <<< centre the colour bar\n",
    "                vmax=obj_mean + obj_std)    #     on the mean range\n",
    "plt.colorbar(hb[3], label=\"Mean objective in bin\")\n",
    "plt.title(\"Performance landscape (outliers clipped, scale ~ mean ±1 σ)\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
