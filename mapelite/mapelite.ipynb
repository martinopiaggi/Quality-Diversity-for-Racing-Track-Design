{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP-Elites track generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import joblib    \n",
    "# Just load a saved t-SNE or UMAP model     \n",
    "EMBEDDING_MODEL = joblib.load(\"embedding/umap_model.joblib\")   # 2-D model trained offline\n",
    "\n",
    "\n",
    "from ribs.archives import SlidingBoundariesArchive\n",
    "from ribs.emitters import EmitterBase\n",
    "from ribs.schedulers import Scheduler\n",
    "from dask.distributed import Client, LocalCluster, as_completed\n",
    "from ribs.visualize import sliding_boundaries_archive_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://localhost:4242'\n",
    "GENERATION_MODE = 'voronoi' #'convexHull' or 'voronoi'\n",
    "POINTS_COUNT = 100\n",
    "MAX_SELECTED_CELLS = 10\n",
    "SOLUTION_DIM = POINTS_COUNT * 2 + MAX_SELECTED_CELLS * 2 + 1 \n",
    "TRACK_SIZE_RANGE = (4, 10)\n",
    "LENGTH_RANGE = (400, 2000)\n",
    "ITERATIONS = 500\n",
    "ARCHIVE_DIM = 3\n",
    "INIT_POPULATION = ARCHIVE_DIM * ARCHIVE_DIM \n",
    "\n",
    "INVALID_SCORE    = -1e9\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "DEBUG_CROSSOVER = True\n",
    "DEBUG_MUTATION = True\n",
    "\n",
    "\n",
    "ARCHIVE_BINS = 30                               # cells per axis\n",
    "REMAPPING_EVERY = 200                           # move boundaries every 200 insertions\n",
    "BUFFER_SIZE = 1000                              # keep last 1000 solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True, n_workers=5, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(iteration):\n",
    "    print(f\"Generating solution for iteration {iteration}\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{BASE_URL}/generate\",\n",
    "            json={\n",
    "                \"id\": iteration + random.random(),\n",
    "                \"mode\": GENERATION_MODE,\n",
    "                \"trackSize\": random.randint(TRACK_SIZE_RANGE[0], TRACK_SIZE_RANGE[1])\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error generating solution for iteration {iteration}: {e}\")\n",
    "        return None\n",
    "\n",
    "def solution_to_array(sol):\n",
    "    if sol is None:\n",
    "        return None\n",
    "    arr = np.zeros(SOLUTION_DIM)\n",
    "    for i, p in enumerate(sol.get(\"dataSet\", [])):\n",
    "        arr[i * 2] = p.get(\"x\", 0)\n",
    "        arr[i * 2 + 1] = p.get(\"y\", 0)\n",
    "    for i, c in enumerate(sol.get(\"selectedCells\", [])):\n",
    "        if i < MAX_SELECTED_CELLS:\n",
    "            idx = POINTS_COUNT * 2 + i * 2\n",
    "            arr[idx] = c.get(\"x\", 0)\n",
    "            arr[idx + 1] = c.get(\"y\", 0)\n",
    "    arr[-1] = sol.get(\"id\", 0)\n",
    "    return arr\n",
    "\n",
    "def array_to_solution(arr):\n",
    "    ds = []\n",
    "    for i in range(0, POINTS_COUNT * 2, 2):\n",
    "        ds.append({\"x\": float(arr[i]), \"y\": float(arr[i+1])})\n",
    "    sel = []\n",
    "    for i in range(POINTS_COUNT * 2, SOLUTION_DIM - 1, 2):\n",
    "        x_val = arr[i]\n",
    "        y_val = arr[i+1]\n",
    "        if x_val != 0 or y_val != 0:\n",
    "            sel.append({\"x\": float(x_val), \"y\": float(y_val)})\n",
    "    return {\n",
    "        \"id\": float(arr[-1]),\n",
    "        \"mode\": GENERATION_MODE,\n",
    "        \"dataSet\": ds,\n",
    "        \"selectedCells\": sel\n",
    "    }\n",
    "\n",
    "def get_fractional_part(x):\n",
    "    return x - int(x)\n",
    "\n",
    "def pca_align(points):\n",
    "    pts = points - points.mean(0)\n",
    "    u, _, _ = np.linalg.svd(pts, full_matrices=False)\n",
    "    angle = np.arctan2(u[1, 0], u[0, 0])\n",
    "    rot = np.array([[np.cos(-angle), -np.sin(-angle)],\n",
    "                    [np.sin(-angle),  np.cos(-angle)]])\n",
    "    aligned = pts @ rot.T\n",
    "    if aligned[0, 0] < 0:\n",
    "        aligned[:, 0] *= -1\n",
    "    return aligned\n",
    "\n",
    "def descriptor_from_track(sol):\n",
    "    pts = np.array([[p[\"x\"], p[\"y\"]] for p in sol.get(\"splineVector\", [])], dtype=float)\n",
    "    aligned = pca_align(pts)\n",
    "    flat = aligned.ravel()\n",
    "    return EMBEDDING_MODEL.transform(flat[None, :])[0]\n",
    "\n",
    "def fitness_formula(fit):\n",
    "    s  = fit[\"speed_entropy\"]\n",
    "    ov = fit[\"total_overtakes\"]\n",
    "    dx = abs(fit[\"deltaX\"])          # use magnitude only\n",
    "    dx = max(dx, 1e-3)               # avoid divide-by-zero blow-ups\n",
    "    return s + ov / dx\n",
    "\n",
    "def evaluate_solution(sol):\n",
    "    sol_id = sol.get(\"id\", 0)\n",
    "    ok = True\n",
    "    msg = \"\"\n",
    "    score = -9999\n",
    "    try:\n",
    "        r = requests.post(f\"{BASE_URL}/evaluate\", json=sol, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        r_json = r.json()\n",
    "        fit = r_json.get(\"fitness\", {})\n",
    "        score = fitness_formula(fit)\n",
    "        desc = descriptor_from_track(r_json)  \n",
    "    except Exception as e:\n",
    "        ok = False\n",
    "        msg = str(e)\n",
    "        desc = np.zeros((2,))  # just in case of error\n",
    "\n",
    "    return sol_id, ok, msg, score, desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmitter(EmitterBase):\n",
    "    def __init__(self, archive, solution_dim, batch_size=ARCHIVE_DIM, bounds=None):\n",
    "        super().__init__(archive, solution_dim=solution_dim, bounds=bounds)\n",
    "        self.batch_size = batch_size\n",
    "        self.iteration = 0\n",
    "\n",
    "    def ask(self):\n",
    "        self.iteration += 1\n",
    "        print(f\"Emitter.ask() called for iteration {self.iteration}\")\n",
    "        if self.iteration <= INIT_POPULATION:\n",
    "            out = []\n",
    "            for _ in range(self.batch_size):\n",
    "                sol = generate_solution(self.iteration - 1)\n",
    "                arr = solution_to_array(sol)\n",
    "                if arr is not None:\n",
    "                    out.append(arr)\n",
    "                else:\n",
    "                    out.append(np.full(SOLUTION_DIM, -9999))\n",
    "            return np.array(out)\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                return self.mutate_solutions()\n",
    "            else:\n",
    "                return self.crossover_solutions()\n",
    "\n",
    "    def mutate_solutions(self):\n",
    "        print(f\"Mutating solutions for iteration {self.iteration}\")\n",
    "        parents = self.archive.sample_elites(self.batch_size)\n",
    "        out = []\n",
    "        for i in range(self.batch_size):\n",
    "            arr = parents[\"solution\"][i]\n",
    "            sol = array_to_solution(arr)\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    f\"{BASE_URL}/mutate\",\n",
    "                    json={\n",
    "                        \"individual\": sol,\n",
    "                        \"intensityMutation\": 10\n",
    "                    },\n",
    "                    timeout=60\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                mutated = response.json().get(\"mutated\", {})\n",
    "                frac = get_fractional_part(sol[\"id\"])\n",
    "                mutated[\"id\"] = self.iteration - 1 + frac\n",
    "                mutated_arr = solution_to_array(mutated)\n",
    "                if mutated_arr is not None:\n",
    "                    out.append(mutated_arr)\n",
    "                    print(f\"Mutated ID={sol['id']} to ID={mutated['id']}\")\n",
    "                else:\n",
    "                    out.append(np.full(SOLUTION_DIM, -9999))\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error mutating solution ID={sol['id']}: {e}\")\n",
    "                out.append(np.full(SOLUTION_DIM, -9999))\n",
    "        return np.array(out)\n",
    "\n",
    "    def crossover_solutions(self):\n",
    "        print(f\"Crossover solutions for iteration {self.iteration}\")\n",
    "        out = []\n",
    "        for _ in range(self.batch_size // 2):\n",
    "            try:\n",
    "                while True:\n",
    "                    parents = self.archive.sample_elites(2)\n",
    "                    sol1 = array_to_solution(parents[\"solution\"][0])\n",
    "                    sol2 = array_to_solution(parents[\"solution\"][1])\n",
    "                    if sol1[\"id\"] != sol2[\"id\"]:\n",
    "                        break\n",
    "                response = requests.post(\n",
    "                    f\"{BASE_URL}/crossover\",\n",
    "                    json={\n",
    "                        \"mode\": GENERATION_MODE,\n",
    "                        \"parent1\": sol1,\n",
    "                        \"parent2\": sol2\n",
    "                    },\n",
    "                    timeout=60\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                offspring = response.json().get(\"offspring\", {})\n",
    "                f1 = get_fractional_part(sol1[\"id\"])\n",
    "                f2 = get_fractional_part(sol2[\"id\"])\n",
    "                frac = (f1 + f2) % 1\n",
    "                child_id = self.iteration - 1 + frac\n",
    "                child_sol = {\n",
    "                    \"id\": child_id,\n",
    "                    \"mode\": GENERATION_MODE,\n",
    "                    \"trackSize\": len(offspring.get(\"sel\", [])),\n",
    "                    \"dataSet\": offspring.get(\"ds\", []),\n",
    "                    \"selectedCells\": offspring.get(\"sel\", [])\n",
    "                }\n",
    "                child_arr = solution_to_array(child_sol)\n",
    "                if child_arr is not None:\n",
    "                    out.append(child_arr)\n",
    "                    print(f\"Crossover Parent1 ID={sol1['id']}, Parent2 ID={sol2['id']} => Child ID={child_id}\")\n",
    "                else:\n",
    "                    out.append(np.full(SOLUTION_DIM, -9999))\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error during crossover: {e}\")\n",
    "                out.append(np.full(SOLUTION_DIM, -9999))\n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illuminating search spaces by mapping elites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "archive = SlidingBoundariesArchive(\n",
    "    solution_dim=SOLUTION_DIM,\n",
    "    dims=[ARCHIVE_BINS, ARCHIVE_BINS],          # 2‑D descriptor space\n",
    "    ranges=[(-1, 1), (-1, 1)],                  # initial bounds in each dim\n",
    "    remap_frequency=REMAPPING_EVERY,\n",
    "    buffer_capacity=BUFFER_SIZE\n",
    ")\n",
    "\n",
    "emitter = CustomEmitter(\n",
    "    archive,\n",
    "    solution_dim=SOLUTION_DIM,\n",
    "    batch_size=INIT_POPULATION,\n",
    "    bounds=[(0, 600)] * (SOLUTION_DIM - 1) + [(0, float('inf'))]\n",
    ")\n",
    "\n",
    "scheduler = Scheduler(archive, [emitter])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Resume from latest checkpoint (if any)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "checkpoint_files = sorted(glob.glob(\"checkpoint_*.npz\"))\n",
    "start_iter = 0\n",
    "if checkpoint_files:\n",
    "    latest = checkpoint_files[-1]\n",
    "    archive.load(latest)\n",
    "    start_iter = int(os.path.splitext(latest)[0].split(\"_\")[1])\n",
    "    emitter.iteration = start_iter\n",
    "    print(f\"[Resume] Loaded {latest}, continuing from iteration {start_iter+1}\")\n",
    "else:\n",
    "    print(\"[Resume] No checkpoint found – starting fresh.\")\n",
    "\n",
    "\n",
    "def run_map_elites(total_iters, start_iter=0):\n",
    "    global_best_score = INVALID_SCORE\n",
    "    global_best_id    = None\n",
    "    for i in range(start_iter, total_iters):\n",
    "        print(f\"=== Starting iteration {i+1} ===\")\n",
    "        try:\n",
    "            sols      = scheduler.ask()\n",
    "            sol_dicts = [array_to_solution(s) for s in sols]\n",
    "            futs      = [client.submit(evaluate_solution, sol) for sol in sol_dicts]\n",
    "            gathered  = [f.result() for f in as_completed(futs)]\n",
    "\n",
    "            obj_list = []\n",
    "            clean    = []\n",
    "            for sol_id, ok, msg, score, desc in gathered:\n",
    "                if not ok or not np.isfinite(score):\n",
    "                    print(f\"Warning: invalid score for solution ID={sol_id}, reason: {msg}\")\n",
    "                    score = INVALID_SCORE\n",
    "                else:\n",
    "                    print(f\"Solution ID={sol_id} evaluated with score={score:.2f}\")\n",
    "                    if score > global_best_score:\n",
    "                        global_best_score = score\n",
    "                        global_best_id    = sol_id\n",
    "                clean.append((score, desc))\n",
    "                obj_list.append(score)\n",
    "\n",
    "            obj_batch, meas_batch = zip(*clean)\n",
    "            scheduler.tell(list(obj_batch), list(meas_batch))\n",
    "\n",
    "            batch_best = max(obj_list) if obj_list else INVALID_SCORE\n",
    "            print(f\"Iteration {i+1} ended. Best in batch = {batch_best:.2f}\")\n",
    "            if global_best_id is not None:\n",
    "                print(f\"Global Best Score so far: {global_best_score:.2f} (ID={global_best_id})\")\n",
    "\n",
    "            data = archive.data()\n",
    "            if len(data) > 0:\n",
    "                arch_obj = data[\"objective\"]\n",
    "                mean_val = np.mean(arch_obj[arch_obj != INVALID_SCORE])\n",
    "                best_val = np.max(arch_obj[arch_obj != INVALID_SCORE])\n",
    "                cov      = archive.stats.coverage\n",
    "                print(f\"Archive size={len(archive)}, Coverage={cov:.3f}, \"\n",
    "                      f\"Mean={mean_val:.2f}, Best={best_val:.2f}\")\n",
    "            else:\n",
    "                print(\"Archive empty so far\")\n",
    "\n",
    "            if (i + 1) % CHECKPOINT_EVERY == 0:\n",
    "                archive.save(f\"checkpoint_{i+1:04d}.npz\")\n",
    "                print(f\"[Checkpoint] Saved checkpoint_{i+1:04d}.npz\")\n",
    "\n",
    "            # ───────────────── Plot (every 5 iterations) ─────────────────\n",
    "            if (i + 1) % 5 == 0:\n",
    "                arch_obj   = data[\"objective\"]\n",
    "                valid_mask = arch_obj != INVALID_SCORE\n",
    "                if np.any(valid_mask):\n",
    "                    vmin = arch_obj[valid_mask].min()\n",
    "                    vmax = arch_obj[valid_mask].max()\n",
    "                else:                 # no valid points yet\n",
    "                    vmin = vmax = 0.0\n",
    "\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sliding_boundaries_archive_heatmap(\n",
    "                    archive,\n",
    "                    boundary_lw=0.5,\n",
    "                    vmin=vmin,\n",
    "                    vmax=vmax,\n",
    "                )\n",
    "                plt.title(f\"Archive Heat-map – Iteration {i+1}\")\n",
    "                plt.xlabel(\"UMAP-1\")\n",
    "                plt.ylabel(\"UMAP-2\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"archive_heatmap_iter_{i+1}.png\")\n",
    "                plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {i+1}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "run_map_elites(ITERATIONS, start_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_map_elites(ITERATIONS, start_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All iterations complete.\")\n",
    "print(f\"Final archive size={len(archive)}, Coverage={archive.stats.coverage:.3f}\")\n",
    "plt.figure(figsize=(6,5))\n",
    "grid_archive_heatmap(archive)\n",
    "plt.title(\"Final Archive Heatmap\")\n",
    "plt.xlabel(\"Speed Entropy\")\n",
    "plt.ylabel(\"Mean Gaps\")\n",
    "plt.savefig(\"final_archive_heatmap.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
