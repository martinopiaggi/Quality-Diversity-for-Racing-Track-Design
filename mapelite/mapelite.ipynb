{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP-Elites track generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from requests.exceptions import Timeout\n",
    "import random\n",
    "from ribs.archives import GridArchive\n",
    "from ribs.emitters import EmitterBase\n",
    "from ribs.schedulers import Scheduler\n",
    "from ribs.visualize import grid_archive_heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://localhost:4242'\n",
    "POINTS_COUNT = 50\n",
    "MAX_SELECTED_CELLS = 10\n",
    "SOLUTION_DIM = POINTS_COUNT * 2 + MAX_SELECTED_CELLS * 2 + 1 \n",
    "TRACK_SIZE_RANGE = (2, 6)\n",
    "LENGTH_RANGE = (400, 2000)\n",
    "ITERATIONS = 500\n",
    "ARCHIVE_DIM = 8\n",
    "INIT_POPULATION = ARCHIVE_DIM * ARCHIVE_DIM \n",
    "\n",
    "DEBUG_CROSSOVER = True\n",
    "DEBUG_MUTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True, n_workers=5, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(iteration):\n",
    "    response = requests.post(f'{BASE_URL}/generate', json={\n",
    "        \"id\": iteration + random.random(),\n",
    "        \"mode\": \"voronoi\",\n",
    "        \"trackSize\": random.randint(TRACK_SIZE_RANGE[0], TRACK_SIZE_RANGE[1])\n",
    "    })\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "    \n",
    "def solution_to_array(solution):\n",
    "    array = np.zeros(SOLUTION_DIM)\n",
    "    # Fill in dataSet\n",
    "    for i, point in enumerate(solution[\"dataSet\"]):\n",
    "        array[i*2] = point[\"x\"]\n",
    "        array[i*2 + 1] = point[\"y\"]\n",
    "    # Fill in selectedCells\n",
    "    for i, cell in enumerate(solution[\"selectedCells\"]):\n",
    "        if i < MAX_SELECTED_CELLS:\n",
    "            array[POINTS_COUNT*2 + i*2] = cell[\"x\"]\n",
    "            array[POINTS_COUNT*2 + i*2 + 1] = cell[\"y\"]\n",
    "    array[-1] = solution[\"id\"]\n",
    "    return array\n",
    "\n",
    "def array_to_solution(array):\n",
    "    dataSet = [{\"x\": float(array[i]), \"y\": float(array[i+1])} for i in range(0, POINTS_COUNT*2, 2)]\n",
    "    selectedCells = [{\"x\": float(array[i]), \"y\": float(array[i+1])} \n",
    "                     for i in range(POINTS_COUNT*2, SOLUTION_DIM-1, 2) \n",
    "                     if array[i] != 0 or array[i+1] != 0]\n",
    "    \n",
    "    return {\n",
    "        \"id\": float(array[-1]),\n",
    "        \"mode\": \"voronoi\",\n",
    "        \"dataSet\": dataSet,\n",
    "        \"selectedCells\": selectedCells\n",
    "    }\n",
    "\n",
    "def get_fractional_part(id_value):\n",
    "    return id_value - int(id_value)\n",
    "\n",
    "\n",
    "def evaluate_solution(solution):\n",
    "    logging.info(f\"Sending solution ID={solution['id']} to /evaluate\")\n",
    "    logging.debug(f\"Solution payload: {json.dumps(solution, indent=2)}\")\n",
    "    try:\n",
    "        response = requests.post(f'{BASE_URL}/evaluate', json=solution, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        fitness = result['fitness']\n",
    "        \n",
    "        speed_entropy = fitness.get('speed_entropy', 0)\n",
    "        curvature_entropy = fitness.get('curvature_entropy', 0)\n",
    "        gaps_mean = fitness.get('gaps_mean', 0)\n",
    "        \n",
    "        if not all(isinstance(x, (int, float)) for x in [speed_entropy, curvature_entropy, gaps_mean]):\n",
    "            raise ValueError(\"Invalid fitness values received\")\n",
    "            \n",
    "        objective = speed_entropy + curvature_entropy - (0.01 * gaps_mean)\n",
    "        measures = [speed_entropy, gaps_mean]\n",
    "        \n",
    "        return objective, measures\n",
    "    except (requests.RequestException, ValueError) as e:\n",
    "        logging.warning(f\"Error in evaluate_solution for ID={solution['id']}: {e}\")\n",
    "        save_json(solution, f\"error_solution_{solution['id']}.json\")\n",
    "        return float('-inf'), [0, 0]\n",
    "    \n",
    "\n",
    "def format_number(n):\n",
    "    if abs(n) > 1e5 or abs(n) < 1e-5:\n",
    "        return f\"{n:.2e}\"\n",
    "    else:\n",
    "        return f\"{n:.2f}\"\n",
    "    \n",
    "\n",
    "\n",
    "def save_json(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmitter(EmitterBase):\n",
    "    def __init__(self, archive, solution_dim, batch_size=ARCHIVE_DIM, bounds=None):\n",
    "        super().__init__(archive, solution_dim=solution_dim, bounds=bounds)\n",
    "        self.batch_size = batch_size\n",
    "        self.iteration = 0\n",
    "\n",
    "    def ask(self):\n",
    "        self.iteration += 1\n",
    "        if self.iteration <= INIT_POPULATION: \n",
    "            return np.array([solution_to_array(generate_solution(self.iteration-1)) for _ in range(self.batch_size)])\n",
    "        elif np.random.random() < 0.5:  # Mutation\n",
    "            return self.mutate_solutions()\n",
    "        else:  # Crossover\n",
    "            return self.crossover_solutions()\n",
    "\n",
    "    def generate_initial_solutions(self):\n",
    "        return np.array([solution_to_array(generate_solution(self.iteration-1)) for _ in range(self.batch_size)])\n",
    "\n",
    "\n",
    "    def mutate_solutions(self):\n",
    "        parents_data = self.archive.sample_elites(self.batch_size)\n",
    "        mutated = []\n",
    "        global total_mutations\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            solution = parents_data['solution'][i]\n",
    "            parent_dict = array_to_solution(solution)\n",
    "\n",
    "            response = requests.post(f'{BASE_URL}/mutate', json={\n",
    "                \"individual\": parent_dict,\n",
    "                \"intensityMutation\": 10\n",
    "            })\n",
    "            response.raise_for_status()\n",
    "            mutated_dict = response.json()['mutated']\n",
    "            \n",
    "            # Keep the same ID for mutations, just update the iteration part\n",
    "            parent_frac = get_fractional_part(parent_dict['id'])\n",
    "            mutated_dict['id'] = self.iteration - 1 + parent_frac\n",
    "            \n",
    "            mutated.append(solution_to_array(mutated_dict))\n",
    "            if DEBUG_MUTATION:\n",
    "                logger.info(f\"Mutation {i+1}, Parent ID {parent_dict['id']}, Mutated ID {mutated_dict['id']}\")\n",
    "        total_mutations+=1\n",
    "        return np.array(mutated)\n",
    "\n",
    "    def crossover_solutions(self):\n",
    "        offspring = []\n",
    "        global total_crossovers\n",
    "        for i in range(self.batch_size // 2):\n",
    "            while True:\n",
    "                parents = self.archive.sample_elites(2)\n",
    "                parent1 = array_to_solution(parents['solution'][0])\n",
    "                parent2 = array_to_solution(parents['solution'][1])\n",
    "                \n",
    "                if parent1['id'] != parent2['id']:\n",
    "                    break\n",
    "\n",
    "            response = requests.post(f'{BASE_URL}/crossover', json={\n",
    "                \"mode\": \"voronoi\",\n",
    "                \"parent1\": parent1,\n",
    "                \"parent2\": parent2\n",
    "            })\n",
    "            response.raise_for_status()\n",
    "            children = response.json()['offspring']\n",
    "\n",
    "            # Create a new ID based on parents' IDs\n",
    "            parent1_frac = get_fractional_part(parent1['id'])\n",
    "            parent2_frac = get_fractional_part(parent2['id'])\n",
    "            child_frac = (parent1_frac + parent2_frac) % 1  # Ensure it's between 0 and 1\n",
    "            new_id = self.iteration - 1 + child_frac\n",
    "            \n",
    "            child = {\n",
    "                \"id\": new_id,\n",
    "                \"mode\": \"voronoi\",\n",
    "                \"trackSize\": len(children['sel']),\n",
    "                \"dataSet\": children['ds'],\n",
    "                \"selectedCells\": children['sel']\n",
    "            }\n",
    "            \n",
    "            if np.random.random() < 0.1:\n",
    "                save_json(parent1, f\"parent1_{self.iteration}_{i}.json\")\n",
    "                save_json(parent2, f\"parent2_{self.iteration}_{i}.json\")\n",
    "                save_json(child, f\"offspring_{self.iteration}_{i}.json\")\n",
    "            \n",
    "            total_crossovers+=1\n",
    "            offspring.append(solution_to_array(child))\n",
    "            if DEBUG_CROSSOVER:\n",
    "                logger.info(f\"Crossover {i+1}, Child: Parent1 ID {parent1['id']}, Parent2 ID {parent2['id']}, Child ID {new_id}\")\n",
    "        \n",
    "        return np.array(offspring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illuminating search spaces by mapping elites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = GridArchive(\n",
    "    solution_dim=SOLUTION_DIM,\n",
    "    dims=[ARCHIVE_DIM, ARCHIVE_DIM],\n",
    "    ranges=[(0, 1), (0, 100)]  # Speed entropy [0,1], Gaps [0,100]\n",
    ")\n",
    "\n",
    "emitter = CustomEmitter(archive,\n",
    "                            solution_dim=SOLUTION_DIM,\n",
    "                            batch_size=INIT_POPULATION,\n",
    "                            bounds=[(0, 600)] * (SOLUTION_DIM - 1) + [(0, float('inf'))])\n",
    "\n",
    "scheduler = Scheduler(archive, [emitter])\n",
    "\n",
    "total_evaluations = 0\n",
    "total_crossovers = 0\n",
    "total_mutations = 0\n",
    "\n",
    "\n",
    "def run_map_elites(iterations):\n",
    "    for itr in range(iterations):\n",
    "        logging.info(f\"=== Starting iteration {itr + 1} ===\")\n",
    "        try:\n",
    "            solutions = scheduler.ask()\n",
    "\n",
    "            # Convert the numpy arrays to solution dicts\n",
    "            solution_dicts = [array_to_solution(sol) for sol in solutions]\n",
    "\n",
    "            # Map your `evaluate_solution` over the entire batch in parallel\n",
    "            futures = client.map(evaluate_solution, solution_dicts)\n",
    "            results = client.gather(futures)  # returns list of (objective, measures) pairs\n",
    "\n",
    "            # Now parse the results\n",
    "            objectives, measures_list = [], []\n",
    "            for objective, measures in results:\n",
    "                if np.isfinite(objective):\n",
    "                    objectives.append(objective)\n",
    "                    measures_list.append(measures)\n",
    "\n",
    "            if objectives and measures_list:\n",
    "                scheduler.tell(objectives, measures_list)\n",
    "                \n",
    "                # Archive statistics\n",
    "                archive_data = archive.data()\n",
    "                archive_objectives = archive_data['objective']\n",
    "                archive_mean = np.mean(archive_objectives) if len(archive_objectives) > 0 else float('nan')\n",
    "                archive_best = np.max(archive_objectives) if len(archive_objectives) > 0 else float('-inf')\n",
    "\n",
    "                logging.info(\n",
    "                    f\"Iteration {itr + 1}: \"\n",
    "                    f\"Mean Speed Entropy = {format_number(np.mean([m[0] for m in measures_list]))}, \"\n",
    "                    f\"Mean Gaps = {format_number(np.mean([m[1] for m in measures_list]))}, \"\n",
    "                    f\"Batch Best = {format_number(np.max(objectives))}, \"\n",
    "                    f\"Archive Size = {len(archive)}, \"\n",
    "                    f\"Archive Coverage = {archive.stats.coverage:.2f}, \"\n",
    "                    f\"Archive Mean = {format_number(archive_mean)}, \"\n",
    "                    f\"Archive Best = {format_number(archive_best)}\"\n",
    "                )\n",
    "            else:\n",
    "                logging.warning(f\"Iteration {itr + 1}: No valid solutions to add to archive\")\n",
    "\n",
    "            # Plot every 10 iterations\n",
    "            if (itr + 1) % 10 == 0:\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                grid_archive_heatmap(archive)\n",
    "                plt.title(f\"MAP-Elites Archive Heatmap - Iteration {itr + 1}\")\n",
    "                plt.xlabel(\"Speed Entropy\")\n",
    "                plt.ylabel(\"Mean Gaps\")\n",
    "                plt.savefig(f\"archive_heatmap_iter_{itr + 1}.png\")\n",
    "                plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in iteration {itr}: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_map_elites(ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Final Statistics: \"\n",
    "                f\"Evaluations = {total_evaluations}, \"\n",
    "                f\"Crossovers = {total_crossovers}, \"\n",
    "                f\"Mutations = {total_mutations}, \"\n",
    "                f\"Archive size = {len(archive)}, \"\n",
    "                f\"Coverage = {archive.stats.coverage:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "grid_archive_heatmap(archive)\n",
    "plt.title(\"Final MAP-Elites Archive\")\n",
    "plt.xlabel(\"Track Size\")\n",
    "plt.ylabel(\"Track Length\")\n",
    "plt.savefig(\"final_archive_heatmap.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
